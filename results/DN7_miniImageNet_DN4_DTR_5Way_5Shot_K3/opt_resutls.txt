Namespace(dataset_dir='../dataset/miniImageNet/', data_name='miniImageNet', mode='train', resume='', epochs=10, cuda=True, ngpu=1, nc=3, clamp_lower=-0.01, clamp_upper=0.01, print_freq=100, outf='../results/DN7_miniImageNet_DN4_DTR_5Way_5Shot_K3')
DN7_DTR(
  (BACKBONE_2D): FourLayer_64F(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.2, inplace=True)
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): LeakyReLU(negative_slope=0.2, inplace=True)
      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): LeakyReLU(negative_slope=0.2, inplace=True)
      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): LeakyReLU(negative_slope=0.2, inplace=True)
    )
    (criterion): CrossEntropyLoss()
  )
  (DT): XGBHead()
)
===================================== Epoch 0 =====================================
Trainset: 5000
Valset: 500
Testset: 500
Eposide-(0): [100/5000]	Time 0.351 (0.359)	Data 0.001 (0.010)	Loss 1.673 (2.585)	Prec@1 24.000 (28.726)
Eposide-(0): [200/5000]	Time 0.342 (0.354)	Data 0.001 (0.005)	Loss 1.533 (2.095)	Prec@1 34.667 (28.564)
Eposide-(0): [300/5000]	Time 0.327 (0.351)	Data 0.001 (0.004)	Loss 1.431 (1.919)	Prec@1 36.000 (29.608)
Eposide-(0): [400/5000]	Time 0.340 (0.350)	Data 0.001 (0.003)	Loss 1.457 (1.827)	Prec@1 40.000 (30.271)
Eposide-(0): [500/5000]	Time 0.339 (0.349)	Data 0.001 (0.003)	Loss 1.666 (1.774)	Prec@1 38.667 (30.677)
Eposide-(0): [600/5000]	Time 0.337 (0.348)	Data 0.001 (0.002)	Loss 1.547 (1.733)	Prec@1 36.000 (31.221)
Eposide-(0): [700/5000]	Time 0.375 (0.348)	Data 0.001 (0.002)	Loss 1.435 (1.698)	Prec@1 37.333 (32.053)
Eposide-(0): [800/5000]	Time 0.373 (0.348)	Data 0.001 (0.002)	Loss 1.318 (1.669)	Prec@1 45.333 (32.964)
Eposide-(0): [900/5000]	Time 0.347 (0.348)	Data 0.001 (0.002)	Loss 1.364 (1.647)	Prec@1 45.333 (33.545)
Eposide-(0): [1000/5000]	Time 0.345 (0.347)	Data 0.001 (0.002)	Loss 1.515 (1.628)	Prec@1 28.000 (34.138)
Eposide-(0): [1100/5000]	Time 0.340 (0.347)	Data 0.001 (0.002)	Loss 1.581 (1.610)	Prec@1 30.667 (34.719)
Eposide-(0): [1200/5000]	Time 0.339 (0.347)	Data 0.001 (0.002)	Loss 1.367 (1.598)	Prec@1 42.667 (35.077)
Eposide-(0): [1300/5000]	Time 0.356 (0.346)	Data 0.002 (0.002)	Loss 1.522 (1.584)	Prec@1 34.667 (35.610)
Eposide-(0): [1400/5000]	Time 0.338 (0.346)	Data 0.001 (0.002)	Loss 1.441 (1.571)	Prec@1 48.000 (36.119)
Eposide-(0): [1500/5000]	Time 0.332 (0.346)	Data 0.001 (0.002)	Loss 1.169 (1.560)	Prec@1 54.667 (36.571)
Eposide-(0): [1600/5000]	Time 0.337 (0.346)	Data 0.001 (0.001)	Loss 1.574 (1.551)	Prec@1 25.333 (36.919)
Eposide-(0): [1700/5000]	Time 0.347 (0.345)	Data 0.001 (0.001)	Loss 1.333 (1.541)	Prec@1 48.000 (37.315)
Eposide-(0): [1800/5000]	Time 0.348 (0.345)	Data 0.001 (0.001)	Loss 1.479 (1.531)	Prec@1 36.000 (37.746)
Eposide-(0): [1900/5000]	Time 0.358 (0.345)	Data 0.001 (0.001)	Loss 1.406 (1.520)	Prec@1 44.000 (38.188)
Eposide-(0): [2000/5000]	Time 0.343 (0.345)	Data 0.001 (0.001)	Loss 1.184 (1.512)	Prec@1 52.000 (38.475)
Eposide-(0): [2100/5000]	Time 0.395 (0.345)	Data 0.001 (0.001)	Loss 1.368 (1.504)	Prec@1 57.333 (38.856)
Eposide-(0): [2200/5000]	Time 0.334 (0.345)	Data 0.001 (0.001)	Loss 1.089 (1.496)	Prec@1 56.000 (39.163)
Eposide-(0): [2300/5000]	Time 0.339 (0.345)	Data 0.001 (0.001)	Loss 1.335 (1.489)	Prec@1 50.667 (39.461)
Eposide-(0): [2400/5000]	Time 0.333 (0.345)	Data 0.001 (0.001)	Loss 1.223 (1.481)	Prec@1 54.667 (39.795)
Eposide-(0): [2500/5000]	Time 0.325 (0.344)	Data 0.001 (0.001)	Loss 1.386 (1.476)	Prec@1 36.000 (40.046)
Eposide-(0): [2600/5000]	Time 0.335 (0.345)	Data 0.000 (0.001)	Loss 1.119 (1.469)	Prec@1 54.667 (40.336)
Eposide-(0): [2700/5000]	Time 0.332 (0.344)	Data 0.001 (0.001)	Loss 1.167 (1.463)	Prec@1 49.333 (40.656)
Eposide-(0): [2800/5000]	Time 0.335 (0.344)	Data 0.000 (0.001)	Loss 1.581 (1.456)	Prec@1 46.667 (40.933)
Eposide-(0): [2900/5000]	Time 0.330 (0.344)	Data 0.001 (0.001)	Loss 1.006 (1.449)	Prec@1 61.333 (41.235)
Eposide-(0): [3000/5000]	Time 0.330 (0.344)	Data 0.001 (0.001)	Loss 1.263 (1.444)	Prec@1 46.667 (41.475)
Eposide-(0): [3100/5000]	Time 0.344 (0.344)	Data 0.001 (0.001)	Loss 1.535 (1.439)	Prec@1 40.000 (41.743)
Eposide-(0): [3200/5000]	Time 0.330 (0.344)	Data 0.000 (0.001)	Loss 1.626 (1.434)	Prec@1 36.000 (41.973)
Eposide-(0): [3300/5000]	Time 0.335 (0.344)	Data 0.001 (0.001)	Loss 1.087 (1.429)	Prec@1 57.333 (42.216)
Eposide-(0): [3400/5000]	Time 0.341 (0.344)	Data 0.001 (0.001)	Loss 1.279 (1.423)	Prec@1 49.333 (42.469)
Eposide-(0): [3500/5000]	Time 0.334 (0.344)	Data 0.001 (0.001)	Loss 1.139 (1.418)	Prec@1 60.000 (42.663)
Eposide-(0): [3600/5000]	Time 0.335 (0.344)	Data 0.001 (0.001)	Loss 1.166 (1.413)	Prec@1 60.000 (42.936)
Eposide-(0): [3700/5000]	Time 0.373 (0.344)	Data 0.001 (0.001)	Loss 1.207 (1.408)	Prec@1 49.333 (43.154)
Eposide-(0): [3800/5000]	Time 0.383 (0.344)	Data 0.001 (0.001)	Loss 1.601 (1.403)	Prec@1 40.000 (43.383)
Eposide-(0): [3900/5000]	Time 0.335 (0.344)	Data 0.001 (0.001)	Loss 1.364 (1.398)	Prec@1 58.667 (43.609)
Eposide-(0): [4000/5000]	Time 0.359 (0.344)	Data 0.002 (0.001)	Loss 1.369 (1.393)	Prec@1 36.000 (43.852)
Eposide-(0): [4100/5000]	Time 0.360 (0.344)	Data 0.000 (0.001)	Loss 1.445 (1.388)	Prec@1 40.000 (44.066)
Eposide-(0): [4200/5000]	Time 0.336 (0.344)	Data 0.001 (0.001)	Loss 1.231 (1.384)	Prec@1 50.667 (44.258)
Eposide-(0): [4300/5000]	Time 0.330 (0.344)	Data 0.001 (0.001)	Loss 1.079 (1.379)	Prec@1 57.333 (44.505)
Eposide-(0): [4400/5000]	Time 0.324 (0.344)	Data 0.001 (0.001)	Loss 1.119 (1.374)	Prec@1 56.000 (44.696)
Eposide-(0): [4500/5000]	Time 0.328 (0.344)	Data 0.000 (0.001)	Loss 1.182 (1.370)	Prec@1 54.667 (44.890)
Eposide-(0): [4600/5000]	Time 0.322 (0.344)	Data 0.001 (0.001)	Loss 1.165 (1.366)	Prec@1 58.667 (45.079)
Eposide-(0): [4700/5000]	Time 0.332 (0.343)	Data 0.001 (0.001)	Loss 1.195 (1.361)	Prec@1 49.333 (45.292)
Eposide-(0): [4800/5000]	Time 0.317 (0.343)	Data 0.001 (0.001)	Loss 1.132 (1.356)	Prec@1 57.333 (45.502)
Eposide-(0): [4900/5000]	Time 0.383 (0.343)	Data 0.000 (0.001)	Loss 0.848 (1.352)	Prec@1 66.667 (45.694)
============ validation on the val set ============
Test-(0): [100/500]	Time 0.185 (0.205)	Loss 1.279 (1.443)	Prec@1 56.000 (47.934)
Test-(0): [200/500]	Time 0.192 (0.199)	Loss 1.608 (1.464)	Prec@1 38.667 (47.595)
Test-(0): [300/500]	Time 0.193 (0.198)	Loss 1.740 (1.476)	Prec@1 46.667 (47.451)
Test-(0): [400/500]	Time 0.190 (0.197)	Loss 1.696 (1.476)	Prec@1 41.333 (47.707)
 * Prec@1 47.547 Best_prec1 0.000
============ Testing on the test set ============
Test-(0): [100/500]	Time 0.194 (0.209)	Loss 1.249 (1.562)	Prec@1 52.000 (49.017)
Test-(0): [200/500]	Time 0.186 (0.202)	Loss 0.933 (1.561)	Prec@1 58.667 (49.367)
Test-(0): [300/500]	Time 0.195 (0.200)	Loss 1.592 (1.546)	Prec@1 49.333 (49.581)
Test-(0): [400/500]	Time 0.203 (0.199)	Loss 1.380 (1.527)	Prec@1 50.667 (49.805)
 * Prec@1 49.987 Best_prec1 47.547
===================================== Epoch 1 =====================================
Trainset: 5000
Valset: 500
Testset: 500
Eposide-(5000): [100/5000]	Time 0.324 (0.355)	Data 0.001 (0.010)	Loss 1.461 (1.156)	Prec@1 33.333 (54.614)
Eposide-(5000): [200/5000]	Time 0.332 (0.350)	Data 0.001 (0.005)	Loss 1.314 (1.164)	Prec@1 48.000 (54.289)
Eposide-(5000): [300/5000]	Time 0.325 (0.349)	Data 0.001 (0.004)	Loss 0.882 (1.154)	Prec@1 69.333 (54.489)
Eposide-(5000): [400/5000]	Time 0.335 (0.348)	Data 0.001 (0.003)	Loss 1.393 (1.151)	Prec@1 48.000 (54.567)
